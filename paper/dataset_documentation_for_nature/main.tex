\documentclass[fleqn,10pt]{wlscirep}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{fancyheadings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{lmodern}
\usepackage{outlines}
\usepackage{bm}
\usepackage{xr}
\usepackage{subfig}
\usepackage{algorithm,algpseudocode}
\usepackage{caption}
\usepackage{threeparttable}
\usepackage{multirow}
\usepackage{booktabs, tabularx}
\pagestyle{fancy}
\usepackage{pdflscape}
\usepackage{afterpage}
\usepackage{geometry}
\usepackage{color, colortbl}
\usepackage{xcolor}
\usepackage[T1]{fontenc}
\usepackage{lineno}
\usepackage[authoryear]{natbib}
\usepackage[normalem]{ulem}
\linenumbers

\title{Combining Human Expertise with Artificial Intelligence}

% omitting $\dag$
\author[1,]{Nikhil Agarwal}
\author[2]{Tan DT Bui}
\author[3]{Curtis P. Langlotz}
\author[4]{Matthew P. Lungren}
\author[5]{Alex Moehring}
\author[6]{Anuj Pareek}
\author[7]{Pranav Rajpurkar}
\author[1]{Tobias Salz}
\author[2]{Steven QH Truong}
\author[8]{Manasi Kutwal}
\affil[1]{MIT and NBER, Department of Economics, Cambridge, MA, 02142, United States}
\affil[2]{VinBrain, Hanoi, Vietnam}
\affil[3]{Stanford University, University Medical Line, Stanford, CA 94305, United States}
\affil[4]{Stanford University, Medical Center, Stanford, CA 94305, United States}
\affil[5]{MIT, Sloan School of Management, Cambridge, MA, 02142, United States}
\affil[6]{Stanford University, Center for Artificial Intelligence in Medicine \& Imaging, Stanford, CA 94304, United States}
\affil[7]{Harvard Medical School, Department of Biomedical Informatics, Cambridge, MA 02115, United States}
\affil[8]{MIT Economics, Blueprint Labs, Cambridge, MA, 02142, United States} 
%\affil[5]{Affiliation, department, city, postcode, country}
%\affil[*]{corresponding author(s): Derek Author (corresponding.author@email.example)}
%\affil[$\dag$]{these authors contributed equally to this work}


\begin{abstract}
%This is a manuscript template for Data Descriptor submissions to \emph{Scientific Data} (\href{http://www.nature.com/scientificdata}{http://www.nature.com/scientificdata}). The abstract must be no longer than 170 words, and should succinctly describe the study, the assay(s) performed, the resulting data, and the reuse potential, but should not make any claims regarding new scientific findings. No references are allowed in this section. 

Artificial Intelligence (AI) is often perceived to supplant humans in tasks requiring complex decision-making and reasoning. Contrary to this view, this dataset provides experimental data on human-AI collaboration to emphasize the need to study optimal collaboration between human experts and AI tools. Using a self-designed interface, we collect probabilistic assessments of $\sim$ 280 radiologists on a subset of 324 historical cases under various information and sequence settings. This paper provides insight into the purpose, collection approach, and example use cases of the dataset. The dataset can be used to understand the use of AI in medical diagnosis, and the dataset was used to study topics like how best to combine AI predictions with human input informed by contextual information and to understand potential human biases in using AI. Researchers can use this resource to explore questions regarding the use of AI in healthcare to improve diagnostic quality.
\end{abstract}

\begin{document}

\flushbottom
\maketitle
%  Click the title above to edit the author information and abstract

\thispagestyle{empty}

% \noindent Please note: Abbreviations should be introduced at the first mention in the main text – no abbreviations lists or tables should be included. 

\section*{Background \& Summary}

Advances in AI have raised concerns about the potential displacement of humans. While there is no doubt that AI has advanced enough to perform tasks requiring complex reasoning, machines still cannot perform the range of tasks that humans can \cite{Ng2016-nm}. In the context of radiology, while deep learning has improved precision in image recognition tasks, AI still cannot process clinical history information the way humans can. This complementarity of AI's precision and humans' contextual information necessitates the study of collaboration between these two. 

To study AI-human collaboration, we launched an experiment to collect chest X-ray diagnostic assessments for retrospective patient cases of $\sim280$ radiologists \footnote{Also referred to as participants, experts.} for 10 main thoracic pathologies (104 sub-pathologies conditional on the significant presence of main pathology) under different information conditions and design tracks. We acquired these assessments using a remote interface and through two modes: a) a continuous probabilistic value of pathology prevalence, and b) a  binarized recommendation on treatment/follow-up. 

The dataset contains additional information on time spent by radiologists on cases, clinical history information of the patients and values of two diagnostic standard specifications \footnote{Diagnostic standard is the indicative truth for the patient cases. We use this variable since a definitive diagnosis for most thoracic pathologies does not exist.} defined by the researchers. These diagnostic standard variables are derived from aggregating the assessments of five board-certified radiologists at Mt. Sinai Hospital and by estimating the leave-one-out mean of the probabilities reported by the radiologists in the experiment. 

Existing literature on this topic compares the performance of AI tools with human prediction power but does not explore how humans collaborate with AI. This dataset aims to fill the gap in the AI-expert collaboration field by providing an experimental and quantitative assessment of chest X-rays.

In the following sections, we discuss the summary statistics, example uses, variables, data collection, and augmentation that gives the resulting dataset. 


%(700 words maximum) An overview of the study design, the assay(s) performed, and the created data, including any background information needed to put this study in the context of previous work and the literature. The section should also briefly outline the broader goals that motivated the creation of this dataset and the potential reuse value. We also encourage authors to include a figure that provides a schematic overview of the study and assay(s) design. The Background \& Summary should not include subheadings. This section and the other main body sections of the manuscript should include citations to the literature as needed. 

\section*{Methods}

\subsection*{Data Collection}

The dataset was collected by researchers at the Massachusetts Institute of Technology (MIT) and Harvard University using a remote interface (Refer Appendix \ref{subsec:Appendix:-Experiment-Interface}). Participants for this experiment were recruited from teleradiology companies (first and third design) and the VinMac healthcare system in Vietnam (second design).

Of the 180 radiologists in the dataset, $\sim$17\% are US-based, and $\sim$20\% have a degree from the US. The average experience of radiologists in the dataset is 22 years, with $\sim$38\% working at large clinics. Of the 162 radiologists who responded to the sex question, 62\% are male. Close to 60\% of radiologists in the experiment had some or significant previous experience working with AI tools. 

The participating radiologists received randomized links to the experiment interface, which included training information, consent form, practice cases (only in designs 1 and 3), and the reads for data collection (Refer Appendix [subsec:Appendix:-Experiment-Interface]). The interface was designed to mimic the clinical setting for the participants. While we compensated the radiologists for participating in the experiment, we randomly provided monetary incentives to some radiologists in designs 1 and 3 \footnote{Participants in design 2 did not receive any monetary incentives because of the nature of the recruitment agreement.}.

During the experiment, we collected radiologists' diagnosis on patient cases across four different information environments:

\begin{enumerate}
    \item X-ray only
    \item X-ray with clinical history information
    \item X-ray with AI assistance 
    \item X-ray with AI assistance and clinical history information
\end{enumerate}

Different approaches to collecting this information had their advantages and disadvantages. Therefore, we create three distinct designs (visualized in figure \ref{fig:experiment_design}) to collect data on radiologists’ probabilities for the four information environments. Within each design, radiologists were randomized into different tracks (sequence of information environments). The proposed three hybrid designs collect both within and across subject data.

In each track, every radiologist views one of the 324 historical cases procured from the Stanford Healthcare System. These cases are manually reviewed for public release and contain X-ray and clinical history information. The use of retrospective cases allows us to avoid ethical and other issues that would arise when experimenting in high-stakes settings. For the AI assistance information environment, we provide radiologists with CheXpert, developed by a team of researchers at Stanford University \footnote{CheXpert is a deep learning algorithm trained on 224,316 chest radiographs of 65,240 patients and uses only the X-ray image to predict the prevalence of fourteen thoracic pathologies.}.

Finally, to establish the diagnostic standard for analyzing the quality of the radiologists' assessment, aggregate data on the assessments of five board-certified radiologists at Mt. Sinai hospital with at least ten years of experience and chest radiology as a sub-specialty was collected. In radiology, a definitive diagnostic standard is generally not available because of inherent variability in radiologists and their image-reading experience. We also introduce a diagnostic standard measure constructed using leave-one-out average of radiologists' assessments from the experiment. The reads used to calculate this diagnostic standard were from the treatment arm with clinical history but no AI assistance. Some other diagnostic standard variables in the dataset include the continuous diagnostic standard and the diagnostic standard constructed using log odds ratio. 

\subsubsection*{Designs}

\subsubsection*{Design 1}

In this design, we assign radiologists to a randomized sequence track (twenty-four tracks) of the four information environments. They read 15 patient cases, 60 in total, under every information environment. These cases are read sequentially without a repeat encounter. At the beginning of the experiment, every radiologist reads eight practice cases.

\subsubsection*{Design 2}

After the initial randomization at the track level into different information sequences, the radiologist read sixty patient cases per information environment and 240 cases in total. These cases are read over four sessions of sixty cases each, and every case is read under the four information environments. A minimum two-week washout period is introduced between each session to ensure that radiologists do not recall their/AI predictions from previous reads of the same case. Within every experiment session, 15 patient cases are read under every information environment in batches of 5, with no case repetition within a session.

\subsubsection*{Design 3}

The randomization in this design is based on access to clinical history in the first information environment versus not. To account for order effects from earlier in the experiment access to AI, the participants in this design receive AI access only after reading the same case in the XO or CH information environment. Radiologists diagnose the same 50 cases with and without assistance. Since the cases read with and without clinical history are different, they diagnose 100 cases in total.

\subsection*{Structure}

Every observation in the dataset refers to a radiologist-patient-pathology in a given experiment session. While there are no individual personal identifiers in the dataset, we have defined unique identifiers for each radiologist and patient-case used in the experiment. There is also information on the design type and the sequence in which each case was viewed by the radiologists within each experiment session. The major variables of interest in this dataset are: probability, treat, alg\_pred and the gt\_{*}{*} variables (refer to the variables list in Appendix \ref{subsec:Appendix:-Variables}) that include information collected using the remote interface. 

\input{data_summary}

\section*{Data Records}

The Data Records section should be used to explain each data record associated with this work, including the repository where this information is stored, and to provide an overview of the data files and their formats. Each external data record should be cited numerically in the text of this section, for example \cite{Hao:gidmaps:2014}, and included in the main reference list as described below. A data citation should also be placed in the subsection of the Methods containing the data-collection or analytical procedure(s) used to derive the corresponding record. Providing a direct link to the dataset may also be helpful to readers (\hyperlink{https://doi.org/10.6084/m9.figshare.853801}{https://doi.org/10.6084/m9.figshare.853801}).

Tables should be used to support the data records, and should clearly indicate the samples and subjects (study inputs), their provenance, and the experimental manipulations performed on each (please see 'Tables' below). They should also specify the data output resulting from each data-collection or analytical step, should these form part of the archived record.

\section*{Technical Validation}

%This section presents any experiments or analyses that are needed to support the technical quality of the dataset. This section may be supported by figures and tables, as needed. This is a required section; authors must present information justifying the reliability of their data.

The code for the validation tasks can be found on the \href{https://github.com/mit-econ-ai/radiology_ai_data.}{GitHub repository}. 

\subsection*{Validation of diagnostic standard quality}

Here, we summarize evidence that the diagnostic standard we construct from the five board-certified radiologists from Mount Sinai, who each read all 324 patient cases in the study, is high quality and robust to various decisions an analyst could make. Table \ref{tab:diag_standard_quality} contains summary statistics for the ground truth created using the Mount Sinai radiologists and a leave-one- out internal ground truth calculated using the reads collected during the experiment under the treatment arm with clinical history but no AI assistance. Table \ref{tab:diag_standard_effort} contains additional summary statistics for the five Mount Sinai diagnostic standard labelers, including their average time and number of clicks. We also show the average agreement of the labels with the original radiologist’s read. Taken together, these analyses demonstrate, for the majority of cases, that the diagnostic standard labelers agree with the assessment of the radiologist who originally read the report in a clinical setting, and we can reject that the average probability assessment is equal to 0.5 at the 5% level.

\subsection*{Validation of radiologist read quality}

To benchmark the quality of AI predictions in our sample and the radiologists in our experiment, we compare our participant pool with the AI input using two performance measures in figure \ref{fig:compare_performance}. The first measure is derived from the receiver operating characteristic (ROC) curve, which measures the trade-off between the false positive and the true positive rate of a classifier as the cutoff for classifying a case as positive or negative is varied. It uses only ordinal information about the AI. A classifier that is not better than random has an AUROC value of 0.5 whereas a perfectly predictive classifier has a value of one. The second measure is the root mean squared error (RMSE), that utilizes cardinal information about the AI prediction. A lower RMSE indicates higher performance. We pool the data for top-level pathologies with AI for each radiologist’s reports and for the AI’s prediction.

\subsection*{Additional validation tests}

Additional validation tests can be found in the appendix of our NBER working paper, \cite{Agarwal2023-dn}. In the paper, we find that incentives do not play a significant role in a correct response. We also verify randomization occurred as expected through various balance and randomization tests. For design 2, we also find that the washout strategy was effective - radiologists’ predictions do not move towards the AI prediction if it was provided in a prior session but do if it is provided in the current session.

\section*{Usage Notes}

The dataset is hosted at XYZ link. Our \href{https://github.com/mit-econ-ai/radiology_ai_data}{GitHub repository} provides a detailed description and source code (Python scripts) on how to use this dataset and reproduce the published validation results. The data does not include any individual identifiers, although we do include our unique identifiers for each radiologist and patient-case. 

Using this dataset, one can answer questions such as: 1. How do humans/radiologists use AI assistance? 2. Is contextual information valuable? 3. Do the costs of AI-induced human biases outweigh the benefits? 4. What is the trade-off, if any, between the cost savings on human effort and decision-quality when delegating tasks to AI tools? 5. How do radiologists with different skill levels use AI assistance? These questions are indicative of the nature of research that can be undertaken with this data resource.

\subsection*{Usage for post-experiment questionnaire}

Some questions that can be explored using information from the post-experiment interview include: Do radiologists exert more effort and process information differently for these conditions compared to others? Does the mention of these conditions in the patient history indication affect the diagnosis? Is there consistency in what radiologists say in the post experiment questionnaire and what they actually end up doing? 


%The Usage Notes should contain brief instructions to assist other researchers with reuse of the data. This may include discussion of software packages that are suitable for analyzing the assay data files, suggested downstream processing steps (e.g. normalization, etc.), or tips for integrating or comparing the data records with other datasets. Authors are encouraged to provide code, programs or data-processing workflows if they may help others understand or use the data. Please see our code availability policy for advice on supplying custom code alongside Data Descriptor manuscripts.

%For studies involving privacy or safety controls on public access to the data, this section should describe in detail these controls, including how authors can apply to access the data, what criteria will be used to determine who may access the data, and any limitations on data use. 

\section*{Code availability}

Our \href{https://github.com/mit-econ-ai/radiology_ai_data}{GitHub repository} includes code (Python 3 and Stata 17) for

\begin{itemize}
    \item Data preparation
    \item Technical validation
    \item Summary statistics and post-experimental questionnaire
\end{itemize}

%For all studies using custom code in the generation or processing of datasets, a statement must be included under the heading "Code availability", indicating whether and how the code can be accessed, including any restrictions to access. This section should also include information on the versions of any software used, if relevant, and any specific variables or parameters used to generate, test, or process the current dataset. 

\nocite{*}
\bibliography{Datasheet.bib}

%\section*{Acknowledgements}  (not compulsory)

%Acknowledgements should be brief, and should not include thanks to anonymous referees and editors, or effusive comments. Grant or contribution numbers may be acknowledged.

\section*{Author contributions statement}

N.A. was responsible for 

%Must include all authors, identified by initials, for example: A.A. conceived the experiment(s), A.A. and B.A. conducted the experiment(s), C.A. and D.A. analysed the results. All authors reviewed the manuscript. 

\section*{Competing interests} 

The authors declare no competing interests.

%The corresponding author is responsible for providing a \href{https://www.nature.com/sdata/policies/editorial-and-publishing-policies#competing}{competing interests statement} on behalf of all authors of the paper. This statement must be included in the submitted article file.

\clearpage

\section*{Figures \& Tables}

\input{figures_tables}

\clearpage

\subsection*{Appendix: Variables \label{subsec:Appendix:-Variables} }

\begin{table}[H]
\caption{Variable List}
\input{tables/var_table.tex}
\end{table}

\clearpage

\subsection*{Appendix: Experiment Interface \label{subsec:Appendix:-Experiment-Interface}}

\input{interface.tex}

\end{document}